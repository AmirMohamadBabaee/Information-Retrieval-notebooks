{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be9c9480",
   "metadata": {},
   "source": [
    "# Information Retrieval Project (Phase 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39abca77",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd4ec34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "import math\n",
    "from string import punctuation\n",
    "from collections import defaultdict\n",
    "from parsivar import Normalizer, Tokenizer, FindStems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0387af",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e08e28c",
   "metadata": {},
   "source": [
    "### Structure of dataset\n",
    "- dataset \n",
    "    - docID\n",
    "        - title\n",
    "        - content\n",
    "        - tags\n",
    "        - date\n",
    "        - url\n",
    "        - category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c8d946c",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_path = './IR_data_news_12k.json'\n",
    "\n",
    "def load_docs(docs_path):\n",
    "    result = {}\n",
    "    with open(docs_path) as f:\n",
    "        docs = json.load(f)\n",
    "        for docID, body in docs.items():\n",
    "            result[docID] = {}\n",
    "            result[docID]['title'] = body['title']\n",
    "            result[docID]['content'] = body['content']\n",
    "            result[docID]['url'] = body['url']\n",
    "            result[docID]['length'] = 0\n",
    "    return result\n",
    "\n",
    "docs = load_docs(docs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ab78a6",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e43a69",
   "metadata": {},
   "source": [
    "### Control Preprocessing Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e0e3ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# whether remove stopwords or not\n",
    "stopwords_remove=True\n",
    "\n",
    "# whether stem the terms or not\n",
    "stemming=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89336973",
   "metadata": {},
   "source": [
    "### Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a24c223d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stopwords(stopwords_path):\n",
    "    stopwords_set = None\n",
    "    with open(stopwords_path, 'r') as f:\n",
    "        stopwords_set = set(f.read().split('\\n'))\n",
    "    return stopwords_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0a9b375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(words, persian_stopwords_path='./persian-stopwords.txt'):\n",
    "    persian_stopwords = load_stopwords(persian_stopwords_path)\n",
    "    return [word for word in words if word not in persian_stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cdd26c",
   "metadata": {},
   "source": [
    "### Stacking Preprocessing components to make a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a67cef5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['سلام', 'امروز', 'خواست&خواه', 'تابع', '3', 'امتحان', 'کرد&کن']\n"
     ]
    }
   ],
   "source": [
    "def preprocess(text, stopwords_remove=True, stemming=True):\n",
    "    normalizer = Normalizer()\n",
    "    tokenizer = Tokenizer()\n",
    "    stemmer = FindStems()\n",
    "    \n",
    "    pure_text = re.sub(f'[{punctuation}؟،٪×÷»«]+', '', text)\n",
    "    normal_text = normalizer.normalize(pure_text)\n",
    "    res = tokenizer.tokenize_words(normal_text)\n",
    "    if stemming:\n",
    "        res = list(map(stemmer.convert_to_stem, res))\n",
    "    if stopwords_remove:\n",
    "        res = remove_stopwords(res)\n",
    "    \n",
    "    return res\n",
    "\n",
    "print(preprocess('سلام من امروز می خواهم این تابع را ۳ بار امتحان کنم.', stopwords_remove=stopwords_remove, stemming=stemming))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb3ddc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_contents(docs_dict, stopwords_remove=True, stemming=True):\n",
    "    normalizer = Normalizer()\n",
    "    tokenizer = Tokenizer()\n",
    "    stemmer = FindStems()\n",
    "    \n",
    "    for docID, body in docs_dict.items():\n",
    "        body['content'] = preprocess(body['content'], stopwords_remove, stemming)\n",
    "        \n",
    "    return docs_dict\n",
    "\n",
    "preprocessed_docs = preprocess_contents(load_docs(docs_path), stopwords_remove=stopwords_remove, stemming=stemming)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9423f431",
   "metadata": {},
   "source": [
    "## Score Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb38101e",
   "metadata": {},
   "source": [
    "### TFIDF Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e68e48",
   "metadata": {},
   "source": [
    "$$tfidf(t, d, D) = tf(t, d) \\times idf(t, D) = \\log(1 + f_{t, d}) \\times \\log(\\frac{N}{n_t})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8082e46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tfidf(term_frequency, document_frequency, collection_size):\n",
    "    return math.log(term_frequency + 1) * math.log(collection_size / document_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6cbc1390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(term, document, collection, do_preprocess=False):\n",
    "    if do_preprocess:\n",
    "        term = preprocess(term, stopwords_remove=stopwords_remove, stemming=stemming)[0]\n",
    "    # compute term frequency\n",
    "    term_frequency = document['content'].count(term)\n",
    "    # compute document frequency\n",
    "    document_frequency = sum([1 for _, doc in collection.items() if term in doc['content']])\n",
    "    collection_size = len(collection)\n",
    "    \n",
    "    # compute final tdidf \n",
    "    return compute_tfidf(term_frequency, document_frequency, collection_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2765e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.011570860416470444"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf('خبرگزاری', preprocessed_docs['3'], preprocessed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746328fd",
   "metadata": {},
   "source": [
    "## Positional Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4683bcd4",
   "metadata": {},
   "source": [
    "Structure of Index:\n",
    "- Token\n",
    "    - Total Appearance\n",
    "    - Champion List\n",
    "    - docs\n",
    "        - list of docs\n",
    "            - total incidences in every doc\n",
    "            - list of each position in every doc\n",
    "            - TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d4def55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_index(index, filename):\n",
    "    with open(filename, 'wb') as outp:  # Overwrites any existing file.\n",
    "        pickle.dump(index, outp, pickle.HIGHEST_PROTOCOL)\n",
    "        print(f'index saved in {filename}')\n",
    "    \n",
    "def load_index(filename):\n",
    "    index = None\n",
    "    with open(filename, 'rb') as inp:\n",
    "        index = pickle.load(inp)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de765c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = load_index('index_v2.pkl')\n",
    "dictionary = load_index('dictionary_v2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3294e51d",
   "metadata": {},
   "source": [
    "## Update Index\n",
    "Add `tfidf` score of each term-document in the current index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b18c04a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_index(index, collection_size, champion_lists_ratio=0):\n",
    "    \n",
    "    for token, token_dict in index.items():\n",
    "        document_frequency = len(token_dict['docs'])\n",
    "        champion_list_size = int(champion_lists_ratio * document_frequency)\n",
    "        champion_list_enable = champion_list_size > 5\n",
    "        \n",
    "        if champion_list_enable:\n",
    "            champion_list = []\n",
    "            \n",
    "        for doc in token_dict['docs']:\n",
    "            docID = list(doc.keys())[0]\n",
    "            doc_dict = list(doc.values())[0]\n",
    "            term_frequency = doc_dict['total_inc']\n",
    "            tfidf_score = compute_tfidf(term_frequency, document_frequency, collection_size)\n",
    "            doc_dict['tfidf'] = tfidf_score\n",
    "            preprocessed_docs[docID]['length'] += tfidf_score ** 2\n",
    "            \n",
    "            if champion_list_enable:\n",
    "                champion_list.append((doc, tfidf_score))\n",
    "                \n",
    "        if champion_list_enable:\n",
    "            champion_list.sort(key=lambda x: x[1], reverse=True)\n",
    "            token_dict['champion list'] = champion_list[:champion_list_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6c1fdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "update_index(index, len(preprocessed_docs), champion_lists_ratio=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18b7a7ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_docs['0']['content'].count('پیام')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee114f0",
   "metadata": {},
   "source": [
    "# Similarity Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279962ec",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9a374f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(query, K, champion_list_enable=False):\n",
    "    \n",
    "    scores = {}\n",
    "    \n",
    "    for term in query:\n",
    "        \n",
    "        if champion_list_enable == False:\n",
    "            postings = index[term]['champion list']\n",
    "            document_frequency_query = len(index[term]['docs'])\n",
    "        else:\n",
    "            postings = index[term]['docs']\n",
    "            document_frequency_query = len(postings)\n",
    "            \n",
    "        term_frequency_query = query.count(term)\n",
    "        \n",
    "        weight_term_query = compute_tfidf(term_frequency_query, document_frequency_query, len(preprocessed_docs))\n",
    "        \n",
    "        for doc in postings:\n",
    "            docID = list(doc.keys())[0]\n",
    "            weight_term_doc = doc[docID]['tfidf']\n",
    "            \n",
    "            if scores.get(docID):\n",
    "                scores[docID] += weight_term_query * weight_term_doc\n",
    "            else:\n",
    "                scores[docID] = weight_term_query * weight_term_doc\n",
    "    \n",
    "    docID_list = list(scores.keys())\n",
    "    \n",
    "    for docID in docID_list:\n",
    "        \n",
    "        doc_length = preprocessed_docs[docID]['length'] ** 0.5\n",
    "        scores[docID] = scores[docID] / doc_length\n",
    "        \n",
    "    scores_tuples = list(scores.items())\n",
    "    scores_tuples.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return scores_tuples[:K]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d52186",
   "metadata": {},
   "source": [
    "## Previous Phase Query Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "78797bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def previous_phase_process_query(query):\n",
    "    \n",
    "    intersect = set()\n",
    "    \n",
    "    query_index = dict()\n",
    "    exception_index = dict()\n",
    "    phrase_index = dict()\n",
    "    \n",
    "    token_docID = dict()\n",
    "    exception_token_docID = dict()\n",
    "    phrase_token_docID = dict()\n",
    "    \n",
    "    exception_tokens = list()\n",
    "    phrasal_tokens = list()\n",
    "    \n",
    "    # find tokens immediately after exclamation's mark\n",
    "    exception_tokens = re.findall(r'\\!\\s(\\w+)', query)\n",
    "    # find tokens in between double quotation\n",
    "    phrasal_tokens = re.findall(r'\"([^\"]*)\"', query)\n",
    "    # remove two types of tokens extracted before from main query\n",
    "    raw_tokens = re.sub(r'\\!\\s\\w+', '', query)\n",
    "    raw_tokens = re.sub(r'\"[^\"]*\"', '', raw_tokens)\n",
    "            \n",
    "#     print(raw_tokens)\n",
    "#     print(\" \".join(exception_tokens))\n",
    "#     print(\" \".join(phrasal_tokens))\n",
    "    \n",
    "    # preprocess each type of token in query\n",
    "    if len(raw_tokens):\n",
    "        preprocessed_query = preprocess(raw_tokens)\n",
    "        preprocessed_query = [token for token in preprocessed_query if token in dictionary]\n",
    "        raw_tokens = preprocessed_query\n",
    "    if len(exception_tokens) > 0:\n",
    "        preprocessed_exception = preprocess(\" \".join(exception_tokens))\n",
    "        preprocessed_exception = [token for token in preprocessed_exception if token in dictionary]\n",
    "        exception_tokens = preprocessed_exception\n",
    "    if len(phrasal_tokens):\n",
    "        preprocessed_phrasals = list()\n",
    "        for phrase in phrasal_tokens:\n",
    "            preprocessed_phrasals.append(preprocess(phrase))\n",
    "    \n",
    "    # separate docID related to extracted raw tokens\n",
    "    if len(raw_tokens):\n",
    "        for token in preprocessed_query:\n",
    "            query_index[token] = index[token]\n",
    "            token_docID[token] = set(map(lambda x: int(list(x.keys())[0]), index[token]['docs']))\n",
    "\n",
    "    # separate docID related to extracted negated tokens\n",
    "    if len(exception_tokens) > 0:\n",
    "        for token in preprocessed_exception:\n",
    "            exception_index = index[token]\n",
    "            exception_token_docID[token] = set(map(lambda x: int(list(x.keys())[0]), index[token]['docs']))\n",
    "            \n",
    "    if len(phrasal_tokens):\n",
    "        for phrase in preprocessed_phrasals:\n",
    "            phrase_token_docID[tuple(phrase)] = dict()\n",
    "            for token in phrase:\n",
    "                phrase_index = index[token]\n",
    "                phrase_token_docID[tuple(phrase)][token] = set(map(lambda x: int(list(x.keys())[0]), phrase_index['docs']))\n",
    "         \n",
    "#         print(phrase_token_docID)\n",
    "        phrase_docID = dict()\n",
    "        for phrase in preprocessed_phrasals:\n",
    "            phrase_docID[tuple(phrase)] = set()\n",
    "            phrase_intersect = phrase_token_docID[tuple(phrase)][phrase[0]]\n",
    "            for token in phrase:\n",
    "                phrase_intersect = phrase_intersect.intersection(phrase_token_docID[tuple(phrase)][token])\n",
    "            \n",
    "            for docID in list(phrase_intersect):\n",
    "#                 print(index[phrase[0]]['docs'])\n",
    "                sequential_position = None\n",
    "                for i in range(len(phrase)):\n",
    "                    current_token = phrase[i]\n",
    "                    for doc in index[current_token]['docs']:\n",
    "                        if int(list(doc.keys())[0]) == docID:\n",
    "                            current_position = set(doc[f'{docID}']['position'])\n",
    "                            if i == 0:\n",
    "                                sequential_position = current_position\n",
    "                                continue\n",
    "                            current_position = set(map(lambda x : x - i, current_position))\n",
    "                            sequential_position = sequential_position.intersection(current_position)\n",
    "                            \n",
    "                if len(sequential_position) > 0:\n",
    "                    phrase_docID[tuple(phrase)].add(docID)\n",
    "                    \n",
    "        final_phrase_docID = phrase_docID[tuple(preprocessed_phrasals[0])]\n",
    "        for phrase in preprocessed_phrasals:\n",
    "            final_phrase_docID = final_phrase_docID.intersection(phrase_docID[tuple(phrase)])\n",
    "                \n",
    "    if len(raw_tokens) > 0:\n",
    "        intersect = token_docID[preprocessed_query[0]]\n",
    "        for token in preprocessed_query:\n",
    "            intersect = intersect.intersection(token_docID[token])\n",
    "        \n",
    "    if len(phrasal_tokens) > 0:\n",
    "        if len(intersect) > 0:\n",
    "            intersect = intersect.intersection(final_phrase_docID)\n",
    "        else: \n",
    "            intersect = final_phrase_docID\n",
    "        \n",
    "    if len(exception_tokens) > 0:\n",
    "        for token in  preprocessed_exception:\n",
    "            intersect = intersect - exception_token_docID[token]\n",
    "            \n",
    "#     print(f'intersect: {intersect}')\n",
    "        \n",
    "    related_docID = list(intersect)\n",
    "#     print(related_docID)\n",
    "#     print(query_index)\n",
    "    related_docs = list()\n",
    "    for docID in related_docID:\n",
    "        total_token_inc = 0\n",
    "        if len(raw_tokens) > 0:\n",
    "            for token in preprocessed_query:\n",
    "                for doc in query_index[token]['docs']:\n",
    "                    if int(list(doc.keys())[0]) == docID:\n",
    "                        total_token_inc += doc[f'{docID}']['total_inc']\n",
    "        if len(phrasal_tokens) > 0:\n",
    "            for phrase in preprocessed_phrasals:\n",
    "                for token in phrase:\n",
    "                    for doc in index[token]['docs']:\n",
    "                        if int(list(doc.keys())[0]) == docID:\n",
    "                            total_token_inc += doc[f'{docID}']['total_inc']\n",
    "        related_docs.append((docID, total_token_inc))\n",
    "        \n",
    "    related_docs.sort(key=lambda x : x[1], reverse=True)\n",
    "        \n",
    "    return related_docs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712056fa",
   "metadata": {},
   "source": [
    "## Interface Function to Retrieve Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "258b138e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(query, K=5, champion_list_enable=True, previous_phase=False):\n",
    "    if previous_phase:\n",
    "        results = previous_phase_process_query(query)\n",
    "        results = list(map(lambda x: (str(x[0]), x[1]), results))\n",
    "    else:\n",
    "        results = cosine_similarity(preprocess(query), K, champion_list_enable=champion_list_enable)\n",
    "    for doc in results:\n",
    "        print(f\"docID: {doc[0]}\")\n",
    "        print(f'Score: {doc[1]}')\n",
    "        print(f'Title: {docs[doc[0]][\"title\"]}')\n",
    "        print(f'URL: {docs[doc[0]][\"url\"]}')\n",
    "        print(50*'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03a570d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docID: 10011\n",
      "Score: 0.8348846434455545\n",
      "Title: نایب رئیس مجلس: رئیس‌جمهور بخشنامه تعیین سقف برای به‌کارگیری افراد وزارت کشور را لغو کرد\n",
      "URL: https://www.farsnews.ir/news/14000918000589/نایب-رئیس-مجلس-رئیس‌جمهور-بخشنامه-تعیین-سقف-برای-به‌کارگیری-افراد\n",
      "--------------------------------------------------\n",
      "docID: 9078\n",
      "Score: 0.8339617071189556\n",
      "Title: شهید سلیمانی تراز جدیدی از مسئولیت‌پذیری را تعریف کرد\n",
      "URL: https://www.farsnews.ir/news/14001013000454/شهید-سلیمانی-تراز-جدیدی-از-مسئولیت‌پذیری-را-تعریف-کرد\n",
      "--------------------------------------------------\n",
      "docID: 7446\n",
      "Score: 0.7893949233470711\n",
      "Title: کواکبیان: برای لغو تحریم‌ها باید راستی‌آزمایی و تضمین داشته باشیم\n",
      "URL: https://www.farsnews.ir/news/14001205000409/کواکبیان-برای-لغو-تحریم‌ها-باید-راستی‌آزمایی-و-تضمین-داشته-باشیم\n",
      "--------------------------------------------------\n",
      "docID: 3299\n",
      "Score: 0.7573491262384539\n",
      "Title: بگوویچ سرمربی شاهین بوشهر شد\n",
      "URL: https://www.farsnews.ir/news/14001110000883/بگوویچ-سرمربی-شاهین-بوشهر-شد\n",
      "--------------------------------------------------\n",
      "docID: 10900\n",
      "Score: 0.7250088780503212\n",
      "Title: نظام آموزشی ما نقادی ذهن را پرورش نداده\n",
      "URL: https://www.farsnews.ir/news/14000826000357/نظام-آموزشی-ما-نقادی-ذهن-را-پرورش-نداده\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "process_query('مردم سالاری')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292658bf",
   "metadata": {},
   "source": [
    "# Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8a20c9",
   "metadata": {},
   "source": [
    "## Simple Uni-word query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7ec8a5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docID: 3514\n",
      "Score: 0.11689068166453247\n",
      "Title: آمار بازی ایران-عراق/یوزها زهردار از شیرهای بین النهرین\n",
      "URL: https://www.farsnews.ir/news/14001107000656/آمار-بازی-ایران-عراق-یوزها-زهردار-از-شیرهای-بین-النهرین\n",
      "--------------------------------------------------\n",
      "docID: 4257\n",
      "Score: 0.10919149136317557\n",
      "Title: سرمربی استرالیا: ایران می‌تواند صدرنشین شود/ از علاقمندان به حافظ و سعدی هستم\n",
      "URL: https://www.farsnews.ir/news/14001028001050/سرمربی-استرالیا-ایران-می‌تواند-صدرنشین-شود-از-علاقمندان-به-حافظ-و\n",
      "--------------------------------------------------\n",
      "docID: 345\n",
      "Score: 0.10815923289734948\n",
      "Title: مسابقات قهرمانی آسیا| پیروزی دلچسب دختران هندبالیست جوان ایران مقابل هند\n",
      "URL: https://www.farsnews.ir/news/14001219000570/مسابقات-قهرمانی-آسیا|-پیروزی-دلچسب-دختران-هندبالیست-جوان-ایران-مقابل\n",
      "--------------------------------------------------\n",
      "docID: 9893\n",
      "Score: 0.10333959012113696\n",
      "Title: با حکم رئیس‌جمهور، مختارپور رئیس سازمان اسناد و کتابخانه ملی شد\n",
      "URL: https://www.farsnews.ir/news/14000921000552/با-حکم-رئیس‌جمهور-مختارپور-رئیس-سازمان-اسناد-و-کتابخانه-ملی-شد\n",
      "--------------------------------------------------\n",
      "docID: 2665\n",
      "Score: 0.09824083516093905\n",
      "Title: کامیابی‌نیا در باشگاه 200 تایی‌های پرسپولیس\n",
      "URL: https://www.farsnews.ir/news/14001118000913/کامیابی‌نیا-در-باشگاه-200-تایی‌های-پرسپولیس\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "process_query('ایران')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d8e7eb1",
   "metadata": {},
   "source": [
    "### DocID = 3514\n",
    "![image.png](./images/iran-1.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a613e28e",
   "metadata": {},
   "source": [
    "### DocID = 4257\n",
    "![image.png](./images/iran-2.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb95d94f",
   "metadata": {},
   "source": [
    "### DocID = 345\n",
    "![image.png](./images/iran-3.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c962fc5",
   "metadata": {},
   "source": [
    "### DocID = 9893\n",
    "![image.png](./images/iran-4.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4fc96eb",
   "metadata": {},
   "source": [
    "### DocID = 2665\n",
    "![image.png](./images/iran-5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94d18ca",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "    همانطور که مشاهده می شود، اولویت و ارجحیت به داک های با طول کمتر داده شده است. دلیل این امر، نرمال سازی امتیاز داک ها می باشد. چرا که هر چه تعداد ترم های استفاده شده در داک کمتر باشد، نسبت امتیاز کسب شده توسط کوئری به سایز داک بیشتر خواهد بود. بنابراین بعد از نرمال سازی امتیاز بالاتری خواهد گرفت. در واقع، در این حالت داک هایی برگردانده می شود که از لحاظ سایز به کوئری نزدیک تر باشد.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b6f059",
   "metadata": {},
   "source": [
    "## Simple multi-word query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "699fce51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docID: 3204\n",
      "Score: 0.5000880974631147\n",
      "Title: قطر با غلبه بر بحرین بر بام هندبال آسیا ایستاد\n",
      "URL: https://www.farsnews.ir/news/14001111000880/قطر-با-غلبه-بر-بحرین-بر-بام-هندبال-آسیا-ایستاد\n",
      "--------------------------------------------------\n",
      "docID: 3768\n",
      "Score: 0.47908428060445374\n",
      "Title: زخم‌کاری هندبال ایران به کویتی‌ها/ بلیت قهرمانی جهان رزرو شد\n",
      "URL: https://www.farsnews.ir/news/14001104000462/زخم‌کاری-هندبال-ایران-به-کویتی‌ها-بلیت-قهرمانی-جهان-رزرو-شد\n",
      "--------------------------------------------------\n",
      "docID: 1966\n",
      "Score: 0.4667436573379279\n",
      "Title: مسابقات فوتسال کافا|تیم ملی زیر 19 سال ایران قهرمان شد\n",
      "URL: https://www.farsnews.ir/news/14001128000882/مسابقات-فوتسال-کافا|تیم-ملی-زیر-9-سال-ایران-قهرمان-شد\n",
      "--------------------------------------------------\n",
      "docID: 1895\n",
      "Score: 0.42339854670177685\n",
      "Title: پست جدید برای پدر سرپرستی در بسکتبال ایران\n",
      "URL: https://www.farsnews.ir/news/14001129000649/پست-جدید-برای-پدر-سرپرستی-در-بسکتبال-ایران\n",
      "--------------------------------------------------\n",
      "docID: 3578\n",
      "Score: 0.4232775223794052\n",
      "Title: سرپوشی: هدف‌مان کسب سکوی آسیا است / هندبال ایران رو به پیشرفت است\n",
      "URL: https://www.farsnews.ir/news/14001106000886/سرپوشی-هدف‌مان-کسب-سکوی-آسیا-است--هندبال-ایران-رو-به-پیشرفت-است\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "process_query('قهرمانی تیم ملی ایران')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "32e6864b",
   "metadata": {},
   "source": [
    "### DocID = 3204\n",
    "![image.png](./images/national-football-1-1.png)\n",
    "![image-2.png](./images/national-football-1-2.png)\n",
    "![image-3.png](./images/national-football-1-3.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "48a1756c",
   "metadata": {},
   "source": [
    "### DocID = 3768\n",
    "![image.png](./images/national-football-2-1.png)\n",
    "![image-2.png](./images/national-football-2-2.png)\n",
    "![image-3.png](./images/national-football-2-3.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e263dcfc",
   "metadata": {},
   "source": [
    "### DocID = 1966\n",
    "![image.png](./images/national-football-3-1.png)\n",
    "![image-2.png](./images/national-football-3-2.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "563e22a0",
   "metadata": {},
   "source": [
    "### DocID = 1895\n",
    "![image.png](./images/national-football-4-1.png)\n",
    "![image-2.png](./images/national-football-4-2.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2602020",
   "metadata": {},
   "source": [
    "### DocID = 3578\n",
    "![image.png](./images/national-football-5-1.png)\n",
    "![image-2.png](./images/national-football-5-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335c6fc1",
   "metadata": {},
   "source": [
    "## Complex uni-word query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7b526f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docID: 11229\n",
      "Score: 1.5604113891365365\n",
      "Title: وام کشاورزان استمهال شود\n",
      "URL: https://www.farsnews.ir/news/14000819000217/وام-کشاورزان-استمهال-شود\n",
      "--------------------------------------------------\n",
      "docID: 8920\n",
      "Score: 1.2662751609198915\n",
      "Title: اختصاصی| شورای نگهبان برای بررسی لایحه رتبه‌بندی معلمان از مجلس مهلت خواست/ طرح تسهیل مجوزهای کسب و کار رد شد\n",
      "URL: https://www.farsnews.ir/news/14001019000650/اختصاصی|-شورای-نگهبان-برای-بررسی-لایحه-رتبه‌بندی-معلمان-از-مجلس-مهلت\n",
      "--------------------------------------------------\n",
      "docID: 7587\n",
      "Score: 1.2128913136501058\n",
      "Title: اعلام نظر شورای نگهبان درباره 16 طرح و لایحه\n",
      "URL: https://www.farsnews.ir/news/14001130001158/اعلام-نظر-شورای-نگهبان-درباره-6-طرح-و-لایحه\n",
      "--------------------------------------------------\n",
      "docID: 11210\n",
      "Score: 0.644744725016795\n",
      "Title: درخواست نماینده پلدختر برای استمهال بدهی سیل‌زدگان به بانک‌ها\n",
      "URL: https://www.farsnews.ir/news/14000819000403/درخواست-نماینده-پلدختر-برای-استمهال-بدهی-سیل‌زدگان-به-بانک‌ها\n",
      "--------------------------------------------------\n",
      "docID: 11475\n",
      "Score: 0.6209158688951909\n",
      "Title: رئیسی: هیچ بانکی حق ندارد کارخانه‌ای را تعطیل کند\n",
      "URL: https://www.farsnews.ir/news/14000813000661/رئیسی-هیچ-بانکی-حق-ندارد-کارخانه‌ای-را-تعطیل-کند\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "process_query('استمهال')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d057a933",
   "metadata": {},
   "source": [
    "### DocID = 11229\n",
    "![image.png](./images/complex-1-1.png)\n",
    "![image-2.png](./images/complex-1-2.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "332cc45d",
   "metadata": {},
   "source": [
    "### DocID = 8920\n",
    "![image.png](./images/complex-2.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fdfe567a",
   "metadata": {},
   "source": [
    "### DocID = 7587\n",
    "![image.png](./images/complex-3-1.png)\n",
    "![image-2.png](./images/complex-3-2.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ef71f1d",
   "metadata": {},
   "source": [
    "### DocID = 11210\n",
    "![image.png](./images/complex-4-1.png)\n",
    "![image-2.png](./images/complex-4-2.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be23bdcf",
   "metadata": {},
   "source": [
    "### DocID = 11475\n",
    "![image.png](./images/complex-5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d3171c",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "    همانطور که مشاهده می شود همه داک های بازگشتی دارای لغت استمهال می باشد و ارتباط با این موضوع را داراست.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586b32dc",
   "metadata": {},
   "source": [
    "## Complex multi-word query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e1249f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docID: 11944\n",
      "Score: 0.8607731054762514\n",
      "Title: ایران اجازه تشکیل «شعبه 2 رژیم صهیونیستی» را در مرزهای شمال غرب خود نخواهد داد/ همسایگان بدانند صبر ایران اندازه‌ای دارد\n",
      "URL: https://www.farsnews.ir/news/14000730000363/ایران-اجازه-تشکیل-شعبه-2-رژیم-صهیونیستی-را-در-مرزهای-شمال-غرب-خود\n",
      "--------------------------------------------------\n",
      "docID: 9161\n",
      "Score: 0.6250097968050439\n",
      "Title: سردار سلیمانی، شهید پروژه نرمالیزاسیون سیاسی است\n",
      "URL: https://www.farsnews.ir/news/14001012000802/سردار-سلیمانی-شهید-پروژه-نرمالیزاسیون-سیاسی-است\n",
      "--------------------------------------------------\n",
      "docID: 2864\n",
      "Score: 0.6174407469117398\n",
      "Title: برگزاری نشست ستاد هماهنگی و برنامه‌ریزی جام جهانی فوتبال ۲۰۲۲\n",
      "URL: https://www.farsnews.ir/news/14001116000591/برگزاری-نشست-ستاد-هماهنگی-و-برنامه‌ریزی-جام-جهانی-فوتبال-۲۰۲۲\n",
      "--------------------------------------------------\n",
      "docID: 7180\n",
      "Score: 0.5953748980354714\n",
      "Title: آمریکا اهداف شوم خود را با بحران‌سازی‌ها پیگیری می‌کند\n",
      "URL: https://www.farsnews.ir/news/14001215000649/آمریکا-اهداف-شوم-خود-را-با-بحران‌سازی‌ها-پیگیری-می‌کند\n",
      "--------------------------------------------------\n",
      "docID: 7312\n",
      "Score: 0.5952128207239772\n",
      "Title: نگاه دوگانه برخی جریان‌ها به یمن و اوکراین نشانه سرسپردگی‌شان به آمریکاست\n",
      "URL: https://www.farsnews.ir/news/14001209000540/نگاه-دوگانه-برخی-جریان‌ها-به-یمن-و-اوکراین-نشانه-سرسپردگی‌شان-به\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "process_query('مناقشات سیاسی خاورمیانه')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41ca45f5",
   "metadata": {},
   "source": [
    "### DocID = 11944\n",
    "![image.png](./images/middleeast-1.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65e06c08",
   "metadata": {},
   "source": [
    "### DocID = 9161\n",
    "![image.png](./images/middleeast-2-1.png)\n",
    "![image-2.png](./images/middleeast-2-2.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58179c6e",
   "metadata": {},
   "source": [
    "### DocID = 2864\n",
    "![image.png](./images/middleeast-3.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce434ec6",
   "metadata": {},
   "source": [
    "### DocID = 7180\n",
    "![image.png](./images/middleeast-4.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7b53ae0",
   "metadata": {},
   "source": [
    "### DocID = 7312\n",
    "![image.png](./images/middleeast-5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a62f661",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "    در این کوئری، تنها داک اول دارای کلمه مناقشات می باشد و به دلیل خاص بودن این کلمه، از امتیاز بالایی برخوردار می باشد. بعد از این داک در داک های دیگر به دلیل وجود کلمه خاورمیانه که این کلمه نیز خاص می باشد، داک ها امتیاز بالاتری دارند.\n",
    "    از لحاظ محتوایی، داک سوم، ارتباط کمتری به موضوع دارد و برخلاف کوئری اصلی که دارای محتوای سیاسی می باشد، جنس داک برگشتی از نوع داک های ورزشی می باشد.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4229959b",
   "metadata": {},
   "source": [
    "# Previous Query Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c871036c",
   "metadata": {},
   "source": [
    "## Simple multi-word query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0c98d2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docID: 1633\n",
      "Score: 135\n",
      "Title: پسری با کفش‌های کتانی خاطره‌انگیز| رحیمی: غریبانه وارد ایران شدیم/ برخی‌ دنبال حرف‌های خاله‌زنکی هستند\n",
      "URL: https://www.farsnews.ir/news/14001118000684/پسری-با-کفش‌های-کتانی-خاطره‌انگیز|-رحیمی-غریبانه-وارد-ایران-شدیم-\n",
      "--------------------------------------------------\n",
      "docID: 2641\n",
      "Score: 113\n",
      "Title: انتخاب گزینه‌ای جوان و درعین حال باتجربه/ مومنی‌مقدم چگونه سرمربی تیم جوانان شد؟\n",
      "URL: https://www.farsnews.ir/news/14001118000663/انتخاب-گزینه‌ای-جوان-و-درعین-حال-باتجربه-مومنی‌مقدم-چگونه-سرمربی-تیم\n",
      "--------------------------------------------------\n",
      "docID: 2831\n",
      "Score: 102\n",
      "Title: افشای دلایل جدایی پروفسور از هندبال ایران| فرناندز:  نمی‌خواهم وارد جنگ شوم/ با مسیر فدراسیون و برخی تصمیم‌ها موافق نبودم\n",
      "URL: https://www.farsnews.ir/news/14001116000751/افشای-دلایل-جدایی-پروفسور-از-هندبال-ایران|-فرناندز--نمی‌خواهم-وارد\n",
      "--------------------------------------------------\n",
      "docID: 5054\n",
      "Score: 93\n",
      "Title: جنگ هندبالی‌ها با حریفان تا دندان مسلح در خاک عربستان/ بهر نبردی بی‌امان آماده باش\n",
      "URL: https://www.farsnews.ir/news/14001018000252/جنگ-هندبالی‌ها-با-حریفان-تا-دندان-مسلح-در-خاک-عربستان-بهر-نبردی\n",
      "--------------------------------------------------\n",
      "docID: 163\n",
      "Score: 85\n",
      "Title: گفت و گو با ساکت‌ ترین مربی ایران| حسینی: با جدایی گل‌محمدی برزخی شدم/ قبول نکردم در پرسپولیس تست بدهم\n",
      "URL: https://www.farsnews.ir/news/14001221000302/گفت-و-گو-با-ساکت‌-ترین-مربی-ایران|-حسینی-با-جدایی-گل‌محمدی-برزخی-شدم-\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "process_query('قهرمانی تیم ملی ایران', previous_phase=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e5f9a585",
   "metadata": {},
   "source": [
    "### DocID = 1633\n",
    "![image.png](./images/national-team-1.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4467bcd9",
   "metadata": {},
   "source": [
    "### DocID = 2641\n",
    "![image.png](./images/national-team-2.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd34fbba",
   "metadata": {},
   "source": [
    "### DocID = 2831\n",
    "![image.png](./images/national-team-3.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af626687",
   "metadata": {},
   "source": [
    "### DocID = 5054\n",
    "![image.png](./images/national-team-4.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "558d4bc7",
   "metadata": {},
   "source": [
    "### DocID = 163\n",
    "![image.png](./images/national-team-5-1.png)\n",
    "![image-2.png](./images/national-team-5-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcd8a45",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "    همانطور که مشاهده می شود، ارتباط محتوایی داک اول و آخر به موضوع کمتر می باشد. همچنین در این حالت، برخلاف مدل مبتنی بر tfidf لزوما داک های با سایز کوچک تر ارجح شمرده نمی شود و داک های طولانی تر نیز به عنوان پاسخ بازگردانده می شود.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372187d3",
   "metadata": {},
   "source": [
    "## Complex multi-word query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "807f656b",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_query('مناقشات سیاسی خاورمیانه', previous_phase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375e4bfa",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\">\n",
    "    همانگونه که مشاهده می شود، مدل قبلی پاسخگویی به کوئری توانایی پاسخ به این کوئری را دارا نبود و به همین دلیل در خروجی هیچ داکی برگردانده نشده است.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
